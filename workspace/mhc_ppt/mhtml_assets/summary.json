{
  "刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章": {
    "title": "刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章",
    "html": "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/article.html",
    "images": [
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/opqv3ix6k9E4e64ZzO7uIqE3ZblwIojfmt7u70m59yS1ylFK-hTu6Ra8V_LaWQJ1P4OlUJPdXLfVBtrm3TwRrw_1.png",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/300_1.png",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/0_1.png",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45.webp",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/0_1.jpg",
      "workspace/mhc_ppt/mhtml_assets/刚刚，梁文锋署名，DeepSeek元旦新论文要开启架构新篇章/nLnAiLrrETuU96Aym1ZDNjhMga6Fe1hiYp332DlZsT_u4THJyu8XegVlG723G5FblhAwxLO31iFVMkzq62jS3w_1.svg"
    ],
    "paragraphs": [
      "新年第一天， DeepSeek 发布了一篇新论文，提出了一种名为 mHC （流形约束超连接）的新架构。",
      "该研究旨在解决传统超连接在大规模模型训练中的不稳定性问题，同时保持其显著的性能增益 。",
      "简单来说，DeepSeek 提出的 mHC 通过将传统 Transformer 的单一残差流扩展为多流并行架构，并利用 Sinkhorn-Knopp 算法将连接矩阵约束在双拟随机矩阵流形上，成功解决了超连接（HC）在大规模训练中因破坏恒等映射属性而导致的数值不稳定和信号爆炸问题。",
      "论文标题：mHC: Manifold-Constrained Hyper-Connections",
      "论文地址：https://arxiv.org/pdf/2512.24880",
      "这篇论文的第一作者有三位：Zhenda Xie（解振达）、Yixuan Wei（韦毅轩）、Huanqi Cao。值得注意的是， DeepSeek 创始人 & CEO 梁文锋也在作者名单中。",
      "传统的残差连接（即 Transformer 中的 x + F (x) 结构）凭借「恒等映射」保证了信号无损传输和训练稳定性。但它的瓶颈在于信息通道的宽度受限于隐藏层维度 C。",
      "近期，以字节跳动Seed团队提出的 Hyper-Connections (HC) 为代表的研究，通过扩展残差流宽度和多样化连接模式，拓展了过去十年中广泛应用的残差连接范式。",
      "虽然这些方法带来了显著的性能提升，但但也带来了两个严重问题：",
      "数值不稳定性 ： 原始的 HC 中，连接矩阵是自由学习的，没有约束。这导致信号在经过多层传播后，数值会「爆炸」或「消失」，破坏了恒等映射的特性，模型越深越难训练。",
      "系统开销大 ： 通道变宽意味着显存读写 (I/O) 和通信成本成倍增加，也就是所谓的「显存墙」问题。",
      "从根本上破坏了残差连接固有的恒等映射属性，导致了严重的训练不稳定性和受限的可扩展性，并额外增加了显著的内存访问开销。",
      "为了解决这些挑战，DeepSeek 的研究团队提出了 Manifold-Constrained Hyper-Connections (mHC，流形约束超连接) 。",
      "这是一个通用框架，它将 HC 的残差连接空间投影到一个特定的流形上，以恢复恒等映射属性，同时结合严格的基础设施优化以确保效率。",
      "它的核心目的是： 在保留「加宽残差流」带来的性能提升的同时，解决其导致的训练不稳定和显存消耗过大的问题 。",
      "团队利用 Sinkhorn-Knopp 算法 将残差连接矩阵投影到 Birkhoff 多胞形（双随机矩阵）上。这使得信号传播变为特征的「凸组合」，从数学上严格保证了信号范数的稳定性（能量守恒）。为了抵消加宽通道带来的开销，团队实施了 内核融合、选择性重计算以及扩展的 DualPipe 通信计算重叠策略 。",
      "实证表明，mHC 不仅解决了稳定性问题，且在大规模训练中（如 27B 模型）表现出卓越的可扩展性。 在 n=4 的扩展倍率下，仅增加了 6.7% 的训练时间开销，却换来了显著的性能提升 。mHC 为基础模型的拓扑架构演进指明了方向。",
      "图 1：残差连接范式示意图。 本图对比了以下三种结构设计： (a) 标准残差连接（Residual Connection）； (b) Hyper-Connections (HC)； (c) 我们提出的 Manifold-Constrained Hyper-Connections (mHC)。与无约束的 HC 不同，mHC 专注于优化残差连接空间，通过将矩阵投影到受约束的流形上，以确保稳定性。",
      "具体方法介绍",
      "流形约束超连接 (mHC)",
      "借鉴恒等映射（Identity Mapping）原则，mHC 的核心前提是将残差映射 约束在一个特定的流形上。",
      "虽然原始的恒等映射是通过强制执行 来确保稳定性，但它能从根本上阻止残差流内部的信息交换，而这种交换对于最大化多流架构的潜力至关重要。",
      "因此，该 DeepSeek 团队提出 将残差映射投影到一个流形上 ，既能保持跨层信号传播的稳定性，又能促进残差流之间的相互作用，以保持模型的表达能力（expressivity）。",
      "为此，他们的做法是将 限制为 双拟随机矩阵 （Doubly Stochastic Matrix），即具有非负项且行和与列和均为 1 的矩阵。",
      "形式上，令 表示双拟随机矩阵的流形（也称为 Birkhoff 多胞形），再将 约束在 中，定义为：",
      "其中 1_n 表示全 1 的 n 维向量。",
      "为什么选择双拟随机性？因为其具有多项有利于大规模训练的理论属性：",
      "范数保持 ：其谱范数有界且不超过 1（即 ），这意味着学习到的映射是非扩张的，可有效缓解梯度爆炸问题。",
      "复合封闭性 ：双拟随机矩阵集对矩阵乘法具有封闭性，确保了跨多层的复合残差映射仍保持双拟随机，从而可在整个模型深度上维持稳定性。",
      "几何解释 ：该集合构成了 Birkhoff 多胞形，是排列矩阵集的凸包。这意味着残差映射充当了排列的凸组合，其重复应用会单调地增加跨流的信息混合，起到鲁棒的特征融合作用。",
      "此外，该团队还对输入映射 和输出映射 施加了非负约束，以防止因正负系数复合导致的信号抵消。",
      "参数化与流形投影",
      "本节将详述 mHC 中各映射的计算过程。",
      "给定第 l 层的输入隐藏矩阵 x_l，先将其展平为向量 以保留完整的上下文信息。然后，按照 HC 的原始公式获取动态映射和静态映射：",
      "最终的约束映射通过以下方式获得：",
      "其中 是 Sigmoid 函数。Sinkhorn-Knopp 算子首先通过指数操作确保所有元素为正，然后进行迭代规范化，交替缩放行和列使其和为 1。",
      "DeepSeek 在实验中采用 t_max=20 次迭代。",
      "高效基础设施设计",
      "DeepSeek 还为 mHC 量身定制了基础设施设计，使其在 n=4 时在大模型中的训练开销仅增加 6.7%：",
      "算子融合 (Kernel Fusion)：",
      "重新调整 RMSNorm 的顺序以提高效率，并采用混合精度策略。",
      "开发了统一的算子，将多次扫描和矩阵乘法融合，减少内存带宽瓶颈和算子启动开销。",
      "在单个算子中实现 Sinkhorn-Knopp 迭代及其自定义反向传播。",
      "将 和 的应用与残差合并融合，显著减少了内存读写量。",
      "重计算 (Recomputing)：",
      "为了减轻 n 流设计带来的内存压力，DeepSeek 在前向传播后丢弃 mHC 算子的中间激活，并在反向传播时即时重新计算。",
      "通过推导得出最优重计算块大小 L_r^*，以最小化总内存占用。",
      "DualPipe 中的通信重叠：",
      "扩展了 DualPipe 调度算法，以改善流水线并行阶段边界处的通信与计算重叠在专用高优先级计算流上执行 MLP 层的内核，并避免在注意力层使用持久算子，以防止阻塞通信流并提高设备利用率。",
      "实验",
      "实验设置",
      "研究团队通过语言模型预训练来验证所提方法的有效性，并对基线模型、超连接（HC）以及提出的流形约束超连接（mHC）进行了对比分析。",
      "他们采用了受 DeepSeek-V3 启发的 MoE 架构，训练了四种不同的模型变体，以覆盖不同的评估体系。",
      "具体而言，HC 和 mHC 的扩展率 n 均设置为 4，主要关注点是一个 27B 参数规模的模型。其训练数据集的大小与其参数量成正比，该模型用于展示系统层面的主要结果。在此基础上，他们通过引入使用成比例数据训练的较小的 3B 和 9B 模型来分析计算扩展性，从而观察不同计算规模下的性能趋势。此外，为了专门研究 Token 规模的影响，他们另外训练了一个独立的 3B 模型，该模型在一个固定的 1T Token 的语料库上进行训练。",
      "主要结果",
      "图 5：流形约束超连接 (mHC) 的训练稳定性。 该图展示了：(a) mHC 和 HC 相对于基线模型的训练损失绝对差值；(b) 三种方法在训练过程中的梯度范数。所有实验均基于 27B 参数规模的模型。实验结果表明，mHC 在损失函数和梯度范数两方面均表现出更优的稳定性。",
      "研究团队首先考察 27B 模型的训练稳定性和收敛性。如图 5 (a) 所示，mHC 有效缓解了在 HC 中观察到的训练不稳定问题，与基线模型相比，最终损失降低了 0.021。图 5 (b) 中的梯度范数分析进一步证实了这种稳定性的提升：mHC 表现出明显优于 HC 的行为，保持了与基线模型相当的稳定轮廓。",
      "表 4：27B 模型在系统级基准测试上的结果。 本表对比了基线模型、HC 以及 mHC 在 8 个不同的下游基准测试中的零样本和少样本性能表现。结果显示，mHC 始终优于基线模型，并在大多数基准测试中超越了 HC，证明了其在大规模预训练中的有效性。",
      "表 4 展示了在多种下游基准测试中的性能表现。mHC 带来了全面的提升，一致性地优于基线模型，并在大多数任务上超过了 HC。值得注意的是，与 HC 相比，mHC 进一步增强了模型的推理能力，在 BBH 和 DROP 任务上分别实现了 2.1% 和 2.3% 的性能增益。",
      "规模扩展实验",
      "图 6：mHC 与基线模型的扩展特性对比。 (a) 计算扩展曲线：实线描绘了在不同计算预算下的性能差距。每个点代表模型大小与数据集大小的最优计算配置，涵盖了从 3B、9B 到 27B 参数规模的规模扩展过程。 (b) Token 扩展曲线：展示了 3B 模型在训练过程中的轨迹。每个点代表模型在不同训练 Token 数量下的性能表现。",
      "为了评估该方法的扩展性，研究者报告了在不同规模下 mHC 相对于基线模型的损失改善情况。在图 6 (a) 中，他们绘制了涵盖 3B、9B 和 27B 参数规模的计算规模扩展曲线。其轨迹表明，即使在更高的计算预算下，性能优势依然稳健地得以保持，仅表现出轻微的衰减。",
      "此外，他们在图 6 (b) 中考察了训练过程中的动态变化，展示了 3B 模型的 Token 扩展曲线。总的来看，这些发现验证了 mHC 在大规模场景下的有效性。这一结论在他们内部的大规模训练实验中得到了进一步的证实。",
      "更多详情请参阅原论文。",
      "© THE END",
      "转载请联系本公众号获得授权",
      "投稿或寻求报道：liyazhou@jiqizhixin.com"
    ]
  },
  "梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”": {
    "title": "梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”",
    "html": "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/article.html",
    "images": [
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/opqv3ix6k9E4e64ZzO7uIqE3ZblwIojfmt7u70m59yS1ylFK-hTu6Ra8V_LaWQJ1P4OlUJPdXLfVBtrm3TwRrw_1.png",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/300_1.png",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/0_1.png",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/640_1_2_3_4_5_6_7_8_9_10.webp",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/640_1_2.png",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/640_1_2_3.png",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/640_1_2_3_4_5_6_7_8_9_10_11.webp",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/640_1_2_3_4_5_6_7_8_9_10_11_12.webp",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/640_1_2_3_4_5_6_7_8_9_10_11_12_13.webp",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14.webp",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15.webp",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16.webp",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17.webp",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18.webp",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19.webp",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/0_1.jpg",
      "workspace/mhc_ppt/mhtml_assets/梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”/nLnAiLrrETuU96Aym1ZDNjhMga6Fe1hiYp332DlZsT_u4THJyu8XegVlG723G5FblhAwxLO31iFVMkzq62jS3w_1.svg"
    ],
    "paragraphs": [
      "2026年的第一天，DeepSeek 在 arXiv 上发布了一篇新论文《mHC: Manifold-Constrained Hyper-Connections》，提出了名为 mHC（流形约束超连接） 的宏观架构的创新。",
      "听名字就很抽象，但若简单总结，这是 Transformer最底层组件残差连接（Residual Connection）的一次重要改进。",
      "这不只是一个技术细节的优化，如果把它放到更大的背景下，事情会更有意思，残差连接是2015年何恺明提出的， 此后十年间几乎没有根本性的改动。DeepSeek这次动的，是Transformer架构中最古老、也最基础的那块砖。",
      "另外值得注意的是，DeepSeek创始人梁文锋也出现在论文的19位作者名单中。",
      "十年接力：从何恺明到DeepSeek",
      "要理解mHC的价值，需要先回顾一段技术演进史。",
      "2015年，何恺明等人在微软亚洲研究院提出ResNet（残差网络）。 在此之前，神经网络越深、训练越困难，梯度消失问题几乎无解。残差连接的核心思想很简单：让信息可以\"跳过\"某些层直接传递，用公式表达就是y = x + F(x)。这个设计使训练上百层甚至上千层的网络成为可能，何恺明因此拿下CVPR 2016最佳论文奖。",
      "2017年，Transformer 问世，残差连接成为标配。从 GPT 系列到 Claude，从 Llama 到 DeepSeek，几乎所有主流大模型都建立在这个基础之上。",
      "2024年9月，字节跳动发表Hyper-Connections（超连接）论文。 研究者发现，传统残差连接虽然稳定，但信息通道的宽度受限，就像一条单车道公路，路面再平整，通行能力终究有限。他们提出将单一残差流扩展为多流并行架构，让不同深度的特征通过多条通道交换信息。实验显示，这能显著提升模型性能，在MoE模型上甚至实现了1.8倍的收敛加速。但问题随之而来，训练变得不稳定了。",
      "而DeepSeek最新发布的mHC这篇论文要解决的，正是Hyper-Connections带来的稳定性问题。",
      "从何恺明到字节再到DeepSeek，这是一场跨越十年的接力。",
      "在Reddit上，有网友用一个比喻来解释这段演进：",
      "就像织毛衣。以前我们只用单股线，容易打结也容易断。现在改用多股线一起织，毛衣更结实、花纹更漂亮。但问题是线太多容易乱成一团。所以我们发明了一个智能理线器，让多股线排列整齐，既保留了多股线的优点，又像单股线一样顺滑好织。",
      "翻译一下就是，ResNet是单股线（稳定但通道有限），Hyper-Connections是多股线（性能更强但容易“乱”），mHC就是那个智能理线器。",
      "Hyper-Connections为什么会“翻车”？",
      "原始残差连接之所以稳定，核心在于保持了“恒等映射”属性，信号通过连接后，能量不会被放大。你输入多少，输出就是多少，像一个能量守恒系统。",
      "但Hyper-Connections为了增强表达能力，引入了可学习的连接权重矩阵。这些矩阵打破了恒等映射的约束，导致几个严重问题：",
      "· 信号爆炸：权重矩阵可能让信号每经过一层就被放大，几十上百层累积下来呈指数级增长",
      "· Loss尖峰：训练过程中损失函数突然暴涨，甚至导致训练崩溃",
      "· 规模受限：模型越大、层数越多，问题越严重",
      "这是一个“富人的问题”，只有在训练超大规模模型（比如270亿参数以上）时才会显现。普通研究者可能永远不会遇到，但对DeepSeek这种体量的玩家来说，这是必须解决的工程难题。",
      "mHC的核心创新：给连接矩阵加“数学护栏”",
      "DeepSeek的解决方案是将连接权重矩阵约束在一个特定的数学空间上，双随机矩阵（Doubly Stochastic Matrix）。",
      "它核心是这样：所有元素非负，每一行的元素加起来等于1，每一列的元素加起来也等于1。",
      "为什么这个约束有效？因为当信号通过这样的矩阵变换时，输出实际上是输入各分量的凸组合，可以理解为一种“加权平均”。根据数学性质，凸组合的结果不会超过输入的最大值。换句话说，信号不会被无限放大，能量守恒得到保证。",
      "从数学角度看，双随机矩阵的谱范数恒小于等于1，这意味着对应的线性变换是\"非扩张的\"——无论前向传播还是反向传播，信号都不会被无限放大。",
      "具体实现上，DeepSeek采用了经典的Sinkhorn-Knopp算法：对矩阵交替进行行归一化和列归一化，迭代几次就能收敛到双随机矩阵。论文实验表明，仅需3次迭代就能达到足够精度，而且整个过程可微分，支持端到端训练。",
      "太艰深了？",
      "没关系，重点是，这个方案的优雅之处在于，它没有引入任何新的超参数需要调节，也没有改变模型的表达能力，只是给原本的权重矩阵套上了一个数学上可证明的安全边界。",
      "实验结果验证了这一设计的有效性：在 7B 规模的 Dense 模型训练中，mHC 模型全程没有出现任何 Loss 尖峰。在 MoE 模型上，收敛速度提升了约 1.8 倍。",
      "DeepSeek“秀肌肉”的一种方式",
      "把mHC放到DeepSeek近两年的发展脉络中看，会发现一条主线，在有限资源下，通过架构创新最大化效率。mHC可以说是对此的延续，用数学约束解决工程问题，用架构创新突破资源瓶颈。",
      "值得一提的是，Twitter用户@nathancgy4（Kimi研究员）表示，一位DeepSeek研究员在和他的交流中认为2025年最值得关注的两大架构创新是muon和hyper-connections。前者已被Kimi深度探索，而后者正是mHC的技术根基。这意味着mHC可能只是DeepSeek在这条路上的第一步。",
      "如果mHC被整合进下一代模型，再结合此前的一系列技术和工程创新，我们可能会看到一个在效率、性能和稳定性上全面升级的架构。",
      "法国 AI 研究实验室 Pleias 联合创始人 Alexander Doria 在读完论文后给出了一个评价，这表面上是一篇架构论文，实际上是一篇“秀肌肉”的硬核工程论文。",
      "因为理论上完美的数学方案（Sinkhorn-Knopp 迭代），如果直接跑在现有的训练框架上，会带来巨大的计算延迟和显存开销。",
      "为了让这个“数学护栏”真正落地，DeepSeek 并没有调用现成的库，而是直接手写了底层的 CUDA 内核代码，利用算子融合（Operator Fusion）技术，把复杂的数学计算硬生生塞进了毫秒级的训练循环里。同时，他们采用了激进的“选择性重计算”策略，并在多卡训练中开辟专用计算流来掩盖通信延迟。",
      "这才是前沿实验室（Frontier Lab）的标志——不仅要有算法灵感，还得有能力为了验证这个灵感，把整个训练环境的内核、内存管理、节点通信全部重写一遍。",
      "这种把想法稳定、高效地落地到算力体系中的工程能力，可能就是DeepSeek最大的优势。"
    ]
  },
  "租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂": {
    "title": "租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂",
    "html": "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/article.html",
    "images": [
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/opqv3ix6k9E4e64ZzO7uIqE3ZblwIojfmt7u70m59yS1ylFK-hTu6Ra8V_LaWQJ1P4OlUJPdXLfVBtrm3TwRrw_1.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/300_1.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/0_1.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5.gif",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6.gif",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7.gif",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8.gif",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29.png",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9.gif",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9.webp",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10.webp",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11.webp",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12.webp",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13.webp",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14.webp",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15.webp",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16.webp",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/640_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17.webp",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/0_1.jpg",
      "workspace/mhc_ppt/mhtml_assets/租了8张H100，他成功复现了DeepSeek的mHC，结果比官方报告更炸裂/nLnAiLrrETuU96Aym1ZDNjhMga6Fe1hiYp332DlZsT_u4THJyu8XegVlG723G5FblhAwxLO31iFVMkzq62jS3w_1.svg"
    ],
    "paragraphs": [
      "元旦期间，DeepSeek 发布的 mHC 震撼了整个 AI 社区。",
      "简单来说，DeepSeek 提出的 mHC 通过将传统 Transformer 的单一残差流扩展为多流并行架构，并利用 Sinkhorn-Knopp 算法将连接矩阵约束在双拟随机矩阵流形上，成功解决了超连接（HC）在大规模训练中因破坏恒等映射属性而导致的数值不稳定和信号爆炸问题。更多详情请参阅《 刚刚，梁文锋署名，DeepSeek 元旦新论文要开启架构新篇章 》。",
      "时至今日，这篇让众多读者大呼看不懂的论文依然是技术社区关注的一大焦点。解读分享这篇论文就好像已成为一种技术时尚。",
      "但还有更加硬核的，近日 FlowMode 工程师 Taylor Kolasinski 宣布成功复现了 mHC，并且在测试中还取得了比 DeepSeek 原始论文更好的成绩 ！",
      "评论区也是直呼「不明觉厉」：",
      "目前，Kolasinski 正通过一个 mHC 复现系列博客介绍其复现成果，相关博客已经发布了 2 篇。这里我们进行了整理，以飨读者。",
      "博客 1：https://taylorkolasinski.com/notes/mhc-reproduction/",
      "博客 2：https://taylorkolasinski.com/notes/mhc-reproduction-part2/",
      "博客一：DeepSeek 的 mHC：当残差连接发生爆炸",
      "你使用过的每一个 Transformer 模型都采用了 2016 年以来的同一种残差连接设计。",
      "GPT-5、Claude、Llama、Gemini。在底层，它们做的事情都是一样的：x + F (x)。信息流只有一条，穿过网络，每一层都向其中添加内容。",
      "DeepSeek 提出了一个问题：如果它变得更宽会怎样？",
      "设置",
      "标准残差连接是每一个现代 Transformer 的脊梁。其思路很简单：",
      "其输入原封不动地流过，加上该层的输出。这是一条单一的信息流。进去是什么，出来的就是什么加上一个学习到的更新量。这就是为什么 Transformer 可以深达数百层：梯度有一条干净的向后路径。简单。稳定。自 2016 年以来未曾改变。",
      "超连接（ Hyper-Connections ） 采取了不同的方法。它不再是单一流，而是扩展到 n 条并行流，并带有可学习的混合矩阵：",
      "下图对比了标准残差与超连接：",
      "三个矩阵控制着信息的流动方式：",
      "H_res：信息流在残差路径中如何混合（红色的交叉部分）",
      "H_pre：信息流在进入层之前如何组合",
      "H_post：层的输出如何分配回各个流中",
      "超连接表达能力更强。参数更多，但计算开销几乎可以忽略不计。理论上性能更好。亦可参阅报道《 字节豆包大模型团队突破残差连接局限！预训练收敛最快加速 80% 》。",
      "但问题是什么？那些混合矩阵是不受约束的。它们不仅能路由信号，还能放大信号。",
      "爆炸",
      "在激进的学习率下，作者的复现实验中超连接（HC）的信号放大达到了 7 倍，随后最终崩溃。Amax（行和列绝对值的最大值）衡量了一个矩阵能将信号放大多少。",
      "在 10M 参数的规模下，这也还行。但 DeepSeek 在 27B 参数下观察到了这种情况：",
      "「Amax 增益幅度产生了极值，峰值达到 3000」",
      "你没有看错： 三千倍 的放大。在 27B 参数下，不受约束的 HC 不仅仅是漂移，而是爆炸了。这里的 10M 复现中达到的 9.2 倍正是这种指数级故障的早期预警。",
      "也因此，不受约束的混合矩阵在规模化时会崩溃。微小的放大呈指数级复合。",
      "压力测试： 在激进的学习率下，HC 的信号放大在崩溃前达到了 7 倍。mHC 保持平稳，维持在 1.0。",
      "修复：约束流形",
      "DeepSeek 的修复方案很干净：将混合矩阵约束为 双重随机（doubly stochastic） 。",
      "一个双重随机矩阵具有以下特性：",
      "所有条目非负",
      "行之和为 1",
      "列之和为 1",
      "这意味着混合操作只能对流进行加权平均。它可以路由信息，混洗它，融合它。但它不能放大。",
      "DeepSeek 是如何做到塞？使用 Sinkhorn-Knopp 算法。",
      "该算法非常简单：",
      "从任意矩阵（原始学习到的权重）开始",
      "取指数使所有条目变为正数：P = e^H",
      "归一化行，使每一行之和为 1",
      "归一化列，使每一列之和为 1",
      "重复 3-4 个步骤，直到收敛",
      "就是这样。交替进行行和列的归一化。二十次迭代就足够了。",
      "这个过程是可微分的。梯度可以回传穿过所有二十次迭代。网络学习原始权重 H，而 Sinkhorn 确保实际的混合矩阵始终是双重随机的。",
      "当作者第一次看到这个时，感觉像是作弊。你不是在学习稳定性，而是在强制它。但有些属性不应该被学习；它们应该被保证。",
      "技术说明：严格来说，只有递归矩阵 H_res 需要完整的 Sinkhorn 双重随机处理。它是层层复合误差的那个。输入 / 输出混合器（H_pre，H_post）仅通过 sigmoid 进行有界处理。Sinkhorn 的计算成本只花在最重要的地方。",
      "结果",
      "不同种子的结果（深度 24，3 个种子）",
      "HC 在原始性能上获胜：验证损失 0.88 对 1.12。在 10M 参数下，mHC 约束就像是一种稳定性税；你付出的是表达能力。但在 27B 参数下，这种税是防止你的模型爆炸成 NaN 的唯一手段。",
      "但看看方差。HC 的损失在不同种子间的变化是 mHC 的 3 倍（±0.033 vs ±0.012）。至于 Amax？HC 根据种子的不同在 6.1 到 7.6 之间摆动。mHC 是 1.00。每一个种子。每一次运行。零方差。",
      "在 10M 参数下，这种不稳定性是可以存活的。HC 仍然获胜。但在 27B 参数下，那 6-7 倍的放大变成了 3000 倍。在这个规模下你无法赌博。",
      "深度扩展",
      "作者还扫描了从 6 到 24 层的深度（保持约 11M 的常数参数预算）：",
      "损失随着深度增加而改善，直到不再改善。深度 20 达到了甜蜜点（0.85 验证损失）。",
      "深度 24 略有退步（0.93），这是由于为了将维度缩小到 192 而产生的宽度瓶颈。",
      "Amax 是不可预测的。深度 20 飙升至 9.2 倍。深度 12 达到 6.6 倍。深度 8 保持在 4.3 倍。没有清晰的关系；HC 是混沌的。",
      "实验细节",
      "数据集： TinyShakespeare（约 1M 字符，字符级）",
      "模型： GPT-2 架构，约 10M 参数",
      "训练： 5000 步，AdamW (β1=0.9, β2=0.95)，权重衰减 0.1，余弦 LR 衰减",
      "硬件： Apple M 系列 (MPS)",
      "深度扫描： 8 种配置（6-24 层），调整宽度以维持约 11M 参数",
      "种子变异： 3 个种子（42, 123, 456），深度 24",
      "为什么这很重要",
      "残差连接不仅仅是帮助梯度流动的技巧。它们是一种守恒定律。",
      "在物理学中，守恒定律约束了可能发生的事情，但使预测成为可能。你不能制造永动机，但你可以精确计算球会落在哪里。",
      "残差连接中的恒等映射是类似的。它通过防止任意变换来约束网络，但它保证了稳定性。信号幅度被保留。",
      "HC 打破了守恒；mHC 恢复了它，不是通过回归到恒等映射，而是通过找到一个更丰富的、仍然守恒信号的流形。",
      "2016 年，何恺明等人引入 ResNets 来解决梯度消失问题，确保信号不会消亡。十年后，相反的问题出现了：超连接带来的信号爆炸。恒等映射通过被动的方式解决了第一个问题。mHC 通过强制守恒解决了第二个问题。",
      "每一个残差连接都是一种守恒定律。mHC 强制执行了它。",
      "不是黑客手段，不是技巧。这是一个原则性的约束，使架构能在规模化下工作。",
      "要点总结",
      "流持久性 Bug 让人学会谦卑 。作者的第一个实现看起来是对的。公式与论文相符。代码能跑。但当把输出投影回单一流并在每一层重新扩展它，扼杀了并行架构。「超连接」中的「超」部分实际上没做任何事。三次独立的审计都说「看起来是对的」。Bug 是架构上的，不是数学上的。作者是在问了「等等，层与层之间流动的实际形状是什么？」之后才发现的。",
      "约束不是限制；它们是保证 。双重随机投影强制了稳定性。你不是在学习好的行为。你是在让坏的行为变得不可能。作者表示自己的第一反应是：「这不优雅。这是束缚。」但其实，HC 达到了 7 倍放大才是重点。",
      "无聊的选择能规模化 。标准残差连接自 2016 年以来一直存活，不是因为它们是最优的，而是因为它们是稳定的。HC 表达能力更强但脆弱。mHC 找到了一个中间地带：比标准残差表达能力更强，且带有稳定性保证。",
      "博客 2：10,924 倍：17 亿规模下的不稳定炸弹",
      "下面是 mHC 复现系列的第 2 部分。第 1 部分 展示了 10M 参数量下的不稳定性。现在，要扩大规模了。",
      "在第 1 部分中，作者在 TinyShakespeare 数据集上训练了一个 10M 参数的 Transformer，并目睹了超连接（Hyper-Connections）将信号放大了 9.2 倍。DeepSeek 的论文 报告称在 27B 参数下放大倍数达到了 3000 倍。现在我们也扩大规模看看。",
      "为了这次运行，作者租用了一个 8x H100 的节点。以下是他的发现。",
      "规模跃迁",
      "10924 倍信号放大！这远远超出了 DeepSeek 论文中的 3000 倍 。",
      "实验",
      "这篇博客记录的是作者在三种架构上进行的 18 次实验，包括：",
      "Residual：标准的残差结构，即 x + F (x) 作为基线；",
      "HC：采用无约束混合矩阵的超连接（Hyper-Connections）；",
      "mHC：采用 Sinkhorn 投影的流形超连接（Manifold Hyper-Connections）。",
      "每种架构分别在两种网络深度下进行（32 层和 48 层），并使用三个随机种子（42、123、456），因此每种配置运行 3 次。",
      "所有模型均在 C4 数据集上训练 5000 步，采用 bf16 混合精度。其中 32 层模型参数量为 17.3 亿（1.73B）；48 层模型参数量为 25.4 亿（2.54B）。",
      "主要结果",
      "首先，在 Loss 表现上：所有方法的收敛表现几乎一致。",
      "三种方法最终都收敛到相近的 loss 区间（约 5.4–6.0）。整体学习曲线几乎完全重合：HC 并没有学得更快，mHC 也没有变慢。从实验结果来看，引入 Sinkhorn 投影几乎没有额外代价。",
      "其次，Amax 表现出强烈的不稳定性。Amax 是用来衡量混合矩阵对信号的放大程度，Amax = 1.0 表示对信号不放大（中性）；数值越高，表示信号被放大的程度越强。",
      "实验中发现，在深度为 32 时，HC 的 Amax 值飙升至 6500 倍，并伴随着剧烈的波动，而 mHC 值则稳定保持在 1.0。在深度为 48 时，这种模式再次出现：HC 猛增至 3500 倍，而 mHC 值保持不变。",
      "Scaling Laws",
      "在对 Amax 与模型参数规模进行 log–log 绘制后，可以观察到明显的放大趋势：当模型规模为 1000 万参数时，Amax 约为 9.2 倍；在 17 亿参数规模下，这一数值跃升至 10924 倍；",
      "而公开数据中，DeepSeek 的 270 亿参数模型对应的 Amax 约为 3000 倍。基于趋势线外推，模型规模达到 100 亿参数时，Amax 可能上升至约 50000 倍，在 1000 亿参数量级下，甚至可能接近 400000 倍。",
      "实验结果并未显示出任何自我修正的迹象，相反，随着模型规模扩大，不稳定性呈现出持续加剧的趋势。值得注意的是，该实验中的 17 亿参数模型所表现出的不稳定性，甚至高于参数规模更大的 DeepSeek 模型。",
      "这种差异可能源于架构设计、训练配方或测量方法的不同；批大小、学习率与网络深度之间的相互作用，也使得尺度效应并非严格单调。",
      "尽管具体数值会受到多种因素影响，但这种不稳定性是客观存在的、可以被量化的，而且规模不容忽视。",
      "可复现性",
      "此外，在三个不同的随机种子下，实验都呈现出完全相同的模式：所有 HC 的训练过程都会发生爆炸，而所有 mHC 的训练过程始终保持平稳。不同随机种子下的 loss 曲线几乎完全重合，两种方法的学习速度也一致。",
      "唯一的差别在于模型内部正在发生的事情：HC 在不断积累不稳定性，这种不稳定性可能在任何时刻被引爆；而 mHC 则始终维持着自身的结构完整性。",
      "逐层分析：不稳定性从哪里开始的",
      "这里有一个令人惊讶的发现： 不稳定性始于输入端，而非输出端 。",
      "HC 的第 0 层（可视化图表中的顶行）率先变红，随后其混合矩阵在训练初期就突破了 Amax 2.0，而更深层的网络则保持相对稳定。看起来问题不在于深度，而在于第 0 层 —— 这是唯一一层直接吞吐原始输入的层。",
      "为什么是第 0 层？ 不同于深层网络前面有 LayerNorm 把关，第一个混合矩阵直接面对原始 Embeddings。其他每一层看到的都是经过归一化、变换后的表征，但第 0 层必须硬抗 Embedding 表吐出的任何数值。如果尺度（scale）没有完美匹配，第 0 层就会学习去补偿。",
      "而在 HC 中，「补偿」可能就意味着「放大」。反观 mHC，在所有层级和所有训练步数中都呈现均匀的绿色。Sinkhorn 投影在限制最大值的同时，也完全防止了任何层发生漂移。",
      "信号流：视觉展示",
      "在第 3000 步时，一个进入 HC 网络的信号在输出时被放大了 532 倍。而同样的信号经过 mHC 输出时倍率为 1.000003 倍，本质上保持不变。",
      "LayerNorm 和非线性模块似乎「收拾」了大部分烂摊子，但这意味著它们消耗了模型容量，仅仅是为了去抵消上游制造的混乱。",
      "这正是守恒定律的体现，它表明残差连接应当保持信号的幅度：输入了什么，就应当输出什么（再加上学习到的残差）。",
      "HC 打破了这一规则，任由信号失控螺旋上升，而 mHC 则守住了底线。",
      "压力测试",
      "正常的训练使用了 1e-4 的学习率。如果加大强度会发生什么？作者在 3 倍于正常学习率的条件下进行了压力测试：",
      "深度 64 的模型在 Amax 达到 14765 倍后，开始在 2000 倍到 10000 倍之间剧烈振荡，同时，混合矩阵彻底失控。",
      "反观 mHC，在所有配置、所有学习率下都表现得平坦、稳定且「无聊」，数值始终保持在 1.0。",
      "意料之外：HC 模型并未崩溃",
      "有一个作者没想到的结果：所有的 HC（Hyper-Connections）运行实验都没有崩溃。",
      "信号放大了 14765 倍，在深度 32 时放大了 10924 倍。Loss（损失）没有发散，训练也没有出现 NaN。模型仍在继续学习。",
      "这是一种「定时炸弹」般的场景。不稳定性确实存在，但尚未导致灾难性的失败…… 至少目前还没有。",
      "为什么没炸？作者列举了以下几种可能性：",
      "梯度裁剪力挽狂澜 。将范数裁剪在 1.0 防止了最严重的梯度爆炸，这几乎肯定就是拯救了这次运行的关键。",
      "5000 步还不够 。如果训练时间再长一点，它可能就会爆发。",
      "这些模型还太小 。在 100B（千亿）参数规模下，动力学特性可能会有所不同。",
      "稳妥的解读是： HC 正在积聚不稳定性，在不同条件下可能会被引爆，而 mHC则完全消除了这种风险 。",
      "重访守恒定律",
      "在第 1 部分中，作者将残差连接定义为了一种守恒定律，即「每一个残差连接都是一条守恒定律，mHC 强制执行了它。」",
      "1.7B 参数规模的结果让这一点变得具体：HC 违反了守恒，信号在训练过程中增长了 10000 多倍。而 mHC 强制守恒，信号保持稳定。具体地，",
      "在 10M（一千万）参数时，违反守恒是可以存活的。作者在第 1 部分中看到的 9.2 倍放大虽然烦人，但尚在可控范围内。",
      "在 1.7B（十七亿）参数时，这就是个炸弹。10924 倍的放大意味着一个本该是量级 1 的信号，现在变成了 10924。梯度更新在与这种放大对抗，而优化器必须做额外的工作来补偿网络内部的混乱。",
      "这还仅仅是在 5000 步的时候，如果训练更久、推高学习率、或者扩展到 10B 参数，在某个临界点，炸弹就会引爆。",
      "mHC 不仅仅是降低了不稳定性，而是彻底消除了这种故障模式。",
      "从这次运行中学到了什么",
      "一是，GPU 3 挂了。8 张 H100 中的一张在特定实验中不断报错 CUDA 错误。作者浪费了一个小时调试「代码问题」，才意识到是硬件故障。云端 GPU 是会坏的。",
      "二是，Batch size（批次大小）的限制是真实的。2.5B 参数的 d48 模型无法在 batch size 为 8 时塞进显存。作者不得不降到 batch size 4。这意味着不同深度下的「每步 token 数」不同。",
      "虽然同一深度下 HC 与 mHC 的对比依然有效（batch size 相同），但跨深度的对比就不那么完美了。",
      "要点总结",
      "如果正在实现超连接：",
      "使用 Sinkhorn 投影。这里大概只有 10 行代码，却消除了一种在大规模下感觉真正危险的故障模式。",
      "在训练期间监控 Amax。如果你看到它爬升超过 10 倍，则是在积聚不稳定性。",
      "第 0 层是「金丝雀」（预警指标）。特别密切关注你的输入混合矩阵。如果你的基础模型有一个不稳定的第 0 层，微调期间的词表变更或 Embedding 漂移可能会导致网络不稳定。",
      "该约束没有性能代价。mHC 的 Loss 与 HC 完全一致。",
      "代码和数据",
      "数据是公开的，代码即将发布。",
      "主要实验: wandb.ai/taylorkolasinski/mhc-part2",
      "压力测试: wandb.ai/taylorkolasinski/mhc-part2-stress",
      "作者表示，包含训练脚本的仓库即将推出。W&B 仪表板拥有每次运行的完整配置、指标和系统日志。实验在一个 Lambda Labs 的 8x H100 SXM5 节点上运行，耗时约 17 小时。",
      "下一步计划",
      "目前有两个悬而未决的问题：",
      "HC 真的会失败吗？ 作者看到了 10924 倍的放大，但训练没有发散。这是一种潜在风险，还是说训练时间更长就会导致失败？",
      "Scaling Law 是什么？ 10M → 9.2 倍。1.7B → 10924 倍。到了 10B 会发生什么？",
      "作者想探索 Scaling Law 到 10B 参数，趋势线表明那里可能出现 50000 倍的放大。那个实验技术上已经准备好了，但需要计算预算的大幅提升。",
      "© THE END",
      "转载请联系本公众号获得授权",
      "投稿或寻求报道：liyazhou@jiqizhixin.com"
    ]
  }
}