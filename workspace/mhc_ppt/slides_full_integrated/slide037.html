<!DOCTYPE html>
<html>
<head>
<style>
html { background: #ffffff; }
body {
  width: 720pt; height: 405pt; margin: 0; padding: 0;
  background: #F4F6F6; font-family: Arial, sans-serif;
  display: flex;
}
.slide { width: 720pt; height: 405pt; }
.header { background: #181B24; height: 54pt; display: flex; align-items: center; padding: 0 36pt; border-bottom: 4pt solid #B165FB; }
.header h1 { color: #ffffff; font-size: 18pt; margin: 0; }
.content { padding: 18pt 36pt 26pt 36pt; }
.heading { font-size: 18pt; font-weight: bold; color: #181B24; margin: 0 0 8pt 0; }
.text { font-size: 13pt; color: #333333; line-height: 1.5; margin: 0 0 8pt 0; }
</style>
</head>
<body>
<div class="slide">
  <div class="header"><h1>梁文锋DeepSeek新论文！接棒何恺明和字节，又稳了稳AI的“地基”</h1></div>
  <div class="content">
    <p class="text">1</p>
<p class="text">十年接力：从何恺明到DeepSeek</p>
<p class="text">要理解mHC的价值，需要先回顾一段技术演进史。</p>
<p class="text">2015年，何恺明等人在微软亚洲研究院提出ResNet（残差网络）。 在此之前，神经网络越深、训练越困难，梯度消失问题几乎无解。残差连接的核心思想很简单：让信息可以"跳过"某些层直接传递，用公式表达就是y = x + F(x)。这个设计使训练上百层甚至上千层的网络成为可能，何恺明因此拿下CVPR 2016最佳论文奖。</p>
<p class="text">2017年，Transformer 问世，残差连接成为标配。从 GPT 系列到 Claude，从 Llama 到 DeepSeek，几乎所有主流大模型都建立在这个基础之上。</p>
  </div>
</div>
</body>
</html>